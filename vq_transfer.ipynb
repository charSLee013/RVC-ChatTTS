{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import click\n",
    "import hydra\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "from lightning import LightningModule\n",
    "from loguru import logger\n",
    "from omegaconf import OmegaConf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from fish_speech.models.vits_decoder.lit_module import VITSDecoder\n",
    "from fish_speech.utils.file import AUDIO_EXTENSIONS\n",
    "\n",
    "# register eval resolver\n",
    "OmegaConf.register_new_resolver(\"eval\", eval)\n",
    "\n",
    "\n",
    "def load_model(config_name, checkpoint_path, device=\"cuda\")->VITSDecoder:\n",
    "    hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "    with initialize(version_base=\"1.3\", config_path=\"../../fish_speech/configs\"):\n",
    "        cfg = compose(config_name=config_name)\n",
    "\n",
    "    # 加载decoder模型\n",
    "    model: VITSDecoder = instantiate(cfg.model)\n",
    "    state_dict = torch.load(\n",
    "        checkpoint_path,\n",
    "        map_location=model.device,\n",
    "    )\n",
    "\n",
    "    if \"state_dict\" in state_dict:\n",
    "        state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    logger.info(\"Restored model from checkpoint\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main(\n",
    "    reference_path,\n",
    "    text,\n",
    "    tokenizer = \"fishaudio/fish-speech-1\",\n",
    "    output_path = \"chatts.wav\",\n",
    "    config_name = \"vits_decoder_finetune\",\n",
    "    checkpoint_path = \"checkpoints/vq-gan-group-fsq-2x1024.pth\",\n",
    "    device = \"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    通过参考音频的风格编码，实现音频风格的迁移\n",
    "    \"\"\"\n",
    "    model:VITSDecoder = load_model(config_name, checkpoint_path, device=device)\n",
    "\n",
    "\n",
    "    # 确保参考音频文件是有效的音频格式。\n",
    "    assert (\n",
    "        reference_path.suffix in AUDIO_EXTENSIONS\n",
    "    ), f\"Expected audio file, got {reference_path.suffix}\"\n",
    "    reference_audio, sr = librosa.load(reference_path, sr=model.sampling_rate)\n",
    "    reference_audio = torch.from_numpy(reference_audio).to(model.device).float()\n",
    "    # 将参考音频形状从(1,seq_length)扩展到 (1,seq_length,1)后，使用spec_transform 方法生成参考音频的线性频谱图\n",
    "    # reference_spec 形状是 (1,N,M)\n",
    "    reference_spec = model.spec_transform(reference_audio[None])\n",
    "    # 使用模型的 encode_ref 方法对参考频谱图进行编码，得到参考嵌入\n",
    "    reference_embedding = model.generator.encode_ref(\n",
    "        reference_spec,\n",
    "        torch.tensor([reference_spec.shape[-1]], device=model.device),\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Loaded reference audio from {reference_path}, shape: {reference_audio.shape}\"\n",
    "    )\n",
    "\n",
    "    # Extract text\n",
    "    # 使用指定的 tokenizer 对文本进行编码\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    encoded_text = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    logger.info(f\"Encoded text: {encoded_text.shape}\")\n",
    "    \n",
    "    chatts_quantized = np.load(\"./chatts.npy\")\n",
    "    quantized = torch.from_numpy(chatts_quantized).to(model.device).long()\n",
    "    \n",
    "    logger.info(f\"Restored VQ features: {quantized.shape}\")\n",
    "\n",
    "    # Decode\n",
    "    # 基于量化特征、编码文本和参考嵌入生成新的音频片段\n",
    "    fake_audios = model.generator.decode(\n",
    "        quantized,\n",
    "        torch.tensor([quantized.shape[-1]], device=model.device),\n",
    "        encoded_text,\n",
    "        torch.tensor([encoded_text.shape[-1]], device=model.device),\n",
    "        ge=reference_embedding,\n",
    "    )\n",
    "    logger.info(\n",
    "        f\"Generated audio: {fake_audios.shape}, equivalent to {fake_audios.shape[-1] / model.sampling_rate:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    # Save audio\n",
    "    fake_audio = fake_audios[0, 0].float().cpu().numpy()\n",
    "    sf.write(output_path, fake_audio, model.sampling_rate)\n",
    "    logger.info(f\"Saved audio to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(reference_path=\"zhongli.ogg\",text=\"黄金是璃月的财富，是令璃月的心脏搏动的血液。你是否拥有黄金般闪耀的心，就让我拭目以待吧。\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
